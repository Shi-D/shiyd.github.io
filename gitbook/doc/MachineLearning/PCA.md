[TOC]

# 主成成分分析PCA

## 定义

PCA的思想是将n维特征映射到k维上（k<n），这k维是全新的正交特征。这k维特征称为主成分，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n-k维特征。



## 实例

现在假设有一组数据如下：

![img](https://img-blog.csdn.net/20150304200426812)

行代表了样例，列代表特征，这里有10个样例，每个样例两个特征。可以这样认为，有10篇文档，x是10篇文档中“learn”出现的TF-IDF，y是10篇文档中“study”出现的TF-IDF。

### 1 求均值

**第一步**，分别求x和y的平均值，然后对于所有的样例，都减去对应的均值。这里x的均值是1.81，y的均值是1.91，那么一个样例减去均值后即为（0.69,0.49），得到

![img](https://img-blog.csdn.net/20150304200606033)

### 2 求方差矩阵 

 **第二步**，求特征协方差矩阵，如果数据是3维，那么协方差矩阵是

![img](https://img-blog.csdn.net/20150304200732621)

 这里只有x和y，求解得

![img](https://img-blog.csdn.net/20150304200837236)

**对角线上分别是x和y的方差，非对角线上是协方差。协方差是衡量两个变量同时变化的变化程度。协方差大于0表示x和y若一个增，另一个也增；小于0表示一个增，一个减。如果ｘ和ｙ是统计独立的，那么二者之间的协方差就是０；但是协方差是０，并不能说明ｘ和ｙ是独立的。协方差绝对值越大，两者对彼此的影响越大，反之越小。协方差是没有单位的量，因此，如果同样的两个变量所采用的量纲发生变化，它们的协方差也会产生变化。**

### 3 求协方差特征值和特征向量

**第三步**，求协方差的特征值和特征向量，得到

![img](https://img-blog.csdn.net/20150304201031902)

上面是两个特征值，下面是对应的特征向量，特征值0.0490833989对应特征向量为，这里的特征向量都归一化为单位向量。

### 4 选择特征值最大的k个

**第四步**，将特征值按照从大到小的顺序排序，选择其中最大的k个，然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵。

这里特征值只有两个，我们选择其中最大的那个，这里是1.28402771，对应的特征向量是(-0.677873399, -0.735178656)T。

### 5 投影样本

**第五步**，将样本点投影到选取的特征向量上。假设样例数为m，特征数为n，减去均值后的样本矩阵为DataAdjust(mxn)，协方差矩阵是n*n，选取的k个特征向量组成的矩阵为EigenVectors(nxk)。那么投影后的数据FinalData为

FinalData(10x1) = DataAdjust(10x2矩阵) x 特征向量(-0.677873399, -0.735178656)T

得到的结果是



![img](https://img-blog.csdn.net/20150304201345746)

这样，就将原始样例的n维特征变成了k维，这k维就是原始特征在k维上的投影。

上面的数据可以认为是learn和study特征融合为一个新的特征叫做LS特征，该特征基本上代表了这两个特征。上述过程如下图2描述：

![img](https://img-blog.csdn.net/20150304201447990)

   正号表示预处理后的样本点，斜着的两条线就分别是正交的特征向量（由于协方差矩阵是对称的，因此其特征向量正交），最后一步的矩阵乘法就是将原始样本点分别往特征向量对应的轴上做投影。

