[TOC]

# 生成学习算法

生成学习算法(Generative Learning Algorithms)

## 核心思想

**判别学习算法**，计算条件概率p(y|x;θ)，直接学习从特征X到标签y∈{0,1}的映射。如逻辑回归，寻找一条直线（决策边界）将两类数据集分开，新数据落入哪边就属于哪类。

**生成学习算法**，计算联合概率p(x,y)，或者理解为对p(x|y)和p(y)同时进行建模。对不同类别的数据集分别进行建模，看新输入的数据更符合哪类模型，该数据就属于哪类。

For example, 对动物进行分类，y=1表示是大象，y=0表示是小狗，p(x|y=1)是对大象特征建模后的分布，p(x|y=0)是对小狗特征建模后的分布。

结合**贝叶斯公式**，可以由先验概率p(x|y)和p(y)求出后验概率p(y|x)。

  ![img](https://images2015.cnblogs.com/blog/929166/201611/929166-20161107215605108-650513378.png)

  其中，p(x)=p(x|y=1)p(y=1)+p(x|y=0)p(y=0)。



## 高斯判别分析

**1. 多元正态分布（multivariate normal distribution）**

n维的**多元正态分布（多元高斯分布）**，由参数均值向量μ∈Rn和协方差矩阵Σ∈R(nXn)确定，记作N(μ,Σ)，它的概率密度公式为：
  ![img](https://images2015.cnblogs.com/blog/929166/201611/929166-20161108150437342-1879508379.png)

其中，|Σ|为Σ的行列式，x是高维向量。

对于这个公式其实不用太在意（吴恩达说的），很少会直接用到它。

对于随机变量X~N(μ,Σ)，它的期望为多元正态分布的**均值μ**，协方差为其**协方差矩阵Σ**。

![img](https://images2015.cnblogs.com/blog/929166/201611/929166-20161108150714014-1552782118.png) 

![img](https://images2015.cnblogs.com/blog/929166/201611/929166-20161108150846092-702816511.png) 

**（2）参数：**

* 协方差Σ

- 均值μ

**2.高斯判别分析建模**

举例来讲，就是先对y=0的样本建模得左下角的等高图，再对y=1的样本建模得右上角的等高图。

对实际例子进行建模后的等高图如下，两类样本集拥有相同的协方差Σ，故它们的形状是完全相同的，而均值μ不同，所以位置是不同的。图中的直线表示的是p(y=1|x)=0.5的**决策边界**。

  ![img](https://images2015.cnblogs.com/blog/929166/201611/929166-20161108154856592-984576892.png)

假设输入x为连续值的随机变量，且满足多元正态分布。

  ![img](https://images2015.cnblogs.com/blog/929166/201611/929166-20161108153713202-213157701.png)

于是代入多元高斯分布公式，可以得到p(x|y)的表达式。

  ![img](https://images2015.cnblogs.com/blog/929166/201611/929166-20161108153826514-1641843840.png)

模型的参数为Φ，μ1，μ2，Σ，它的对数似然函数为，这里所求的是**联合概率**。

  ![img](https://images2015.cnblogs.com/blog/929166/201611/929166-20161108154210030-1764660420.png)

用极大似然估计进行参数的估计（<u>这个知识点看自己那本“其他知识”的笔记本的极大似然估计的那一part</u>），可得各个参数的估计值：

  ![img](https://images2015.cnblogs.com/blog/929166/201611/929166-20161108154452202-1209608612.png)

**3.高斯判别和逻辑回归解决分类问题的优劣**

相对于logistic regression而言，高斯判别分析往往需要更少的数据，但高斯判别会对数据作更多的假设，而logistic regression则不会，因此logistic regression更加健壮。



## 朴素贝叶斯方法

略，在你的“其他知识”的笔记本中有讲朴素贝叶斯分类，自己去翻吧~



## 拉普拉斯平滑

单纯用极大似然估计和贝叶斯分类，若某一类样本没出现过，会导致极大似然估计将其概率判为0，这显然不合理。

因此用到拉普拉斯平滑，即对每个数量都加上1。

周志华的《机器学习》p153也有提到拉普拉斯修正法，所以这边我就不写了。





------

*来源：Stanford cs229 吴恩达老师的机器学习课程

*Coursera：https://www.coursera.org/learn/machine-learning/home/welcome

*课程官网：http://cs229.stanford.edu/