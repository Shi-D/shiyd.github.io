[TOC]

# Spectral Clustering



## Supplementary Notes



### 构建邻接矩阵

在谱聚类中，只有数据点的定义，并没有直接给出图（即，没有邻接矩阵𝑊 ）。因此可以通过样本点距离度量的相似矩阵 𝑆 来获得邻接矩阵𝑊 。

构建邻接矩阵 𝑊 的方法有三类。$𝜖$-邻近法，$k$ 邻近法和全连接法。



#### $𝜖$-邻近法

设置了一个距离阈值 $ϵ$，然后用欧式距离 $s_{ij}$ 度量任意两点 $xi$ 和 $xj$ 的距离。即相似矩阵的 $s_{ij}=||x_i−x_j||^2_2$，然后根据 $s_{ij}$ 和 $ϵ$ 的大小关系，来定义邻接矩阵 $W$ 如下：
$$
w_{ij} =
\begin{cases}
0 \,\,s_{ij}>ϵ\\
ϵ \,\,s_{ij}\leϵ\\
\end{cases}
$$
从上式可见，两点间的权重为 ϵ 或者0。距离远近度量很不精确，因此在实际应用中，很少使用 ϵ-邻近法。



#### $k$ 邻近法

利用 KNN 算法遍历所有的样本点，取每个样本最近的 k 个点作为近邻，只有和样本距离最近的 k 个点之间的 $w_{ij}>0$。但是这种方法会造成重构之后的邻接矩阵 W 非对称，而后面的算法需要对称邻接矩阵。同样的，欧式距离$s_{ij}=||x_i−x_j||^2_2$。为了解决这种问题，一般采取下面两种方法之一：

* 第一种 K 邻近法是只要一个点在另一个点的 K 近邻中，则保留 $𝑠_{𝑖𝑗}$

$$
𝑤_{𝑖𝑗} = 𝑤_{𝑗𝑖} =
\begin{cases}
0 \qquad\qquad\qquad 𝑥_𝑖\notin𝐾𝑁𝑁(𝑥_𝑗)\ 𝑎𝑛𝑑 \ 𝑥_𝑗 \notin 𝐾𝑁𝑁(𝑥_𝑖)\\
𝑒𝑥𝑝(−\frac{s_{ij}}{2𝜎^2})\qquad 𝑥_𝑖\in𝐾𝑁𝑁(𝑥_𝑗)\ 𝑜𝑟\ 𝑥_𝑗\in𝐾𝑁𝑁(𝑥_𝑖)
\end{cases}
$$

* 第二种 K 邻近法是必须两个点互为 K 近邻中，才能保留 $𝑠_{𝑖𝑗}$

$$
𝑤_{𝑖𝑗} = 𝑤_{𝑗𝑖} =
\begin{cases}
0 \qquad\qquad\qquad 𝑥_𝑖\notin𝐾𝑁𝑁(𝑥_𝑗)\ or \ 𝑥_𝑗 \notin 𝐾𝑁𝑁(𝑥_𝑖)\\
𝑒𝑥𝑝(−\frac{s_{ij}}{2𝜎^2})\qquad 𝑥_𝑖\in𝐾𝑁𝑁(𝑥_𝑗)\ and\ 𝑥_𝑗\in𝐾𝑁𝑁(𝑥_𝑖)
\end{cases}
$$



#### 全连接法

全连接法所有的点之间的权重值都大于0。可以选择不同的核函数来定义边权重，常用的有**多项式核函数**，**高斯核函数**和 **Sigmoid 核函数**。最常用的是**高斯核函数RBF**，此时相似矩阵和邻接矩阵相同：
$$
𝑤_{𝑖𝑗}=𝑠_{𝑖𝑗}=𝑒𝑥𝑝(−\frac{s_{ij}}{2𝜎^2})
$$


> 在实际的应用中，使用第三种全连接法来建立邻接矩阵是最普遍的，而在全连接法中使用高斯径向核RBF是最普遍的。



### Laplacians 拉普拉斯矩阵

拉普拉斯矩阵 L = 度矩阵 D - 邻接矩阵 A

关于度矩阵：

> 1. 度矩阵在有向图中，只考虑入度或出度即可
> 2. 度矩阵在无向图中，为对角线矩阵

在无向图中，拉普拉斯矩阵有如下性质：

> 1. 因为度矩阵和邻接矩阵均为对称矩阵，因此拉普拉斯矩阵是对称矩阵
>
> 2. 因为拉普拉斯矩阵是对称矩阵，因此它的所有的特征值都是实数
>
> 3. 对于任意的向量𝑓，有
>    $$
>    𝑓^𝑇𝐿𝑓=\frac{1}{2}\sum_{𝑖,𝑗=1}^𝑛𝑤_{𝑖𝑗}(𝑓_𝑖−𝑓_𝑗)^2
>    $$
>
> 4. 由性质3 可得，拉普拉斯矩阵是半正定的，且对应的n个实数特征值都大于等于0，即 0=𝜆~1~≤𝜆~2~≤...≤𝜆~𝑛~， 且最小的特征值为0。



### 无向图切图

无向图 G 的切图，是将图 G(V,E) 切成相互没有连接的 k 个子图，子图的节点集合没有交集，并集为 V 。

任意两个子图A、B间的切图权重为：$𝑊(𝐴,𝐵)=∑_{𝑖∈𝐴,𝑗∈𝐵}𝑤_{𝑖𝑗}$

则，定义切图cut为：
$$
𝑐𝑢𝑡(𝐴_1,𝐴_2,...𝐴_𝑘)=\frac{1}{2}\sum_{i=1}^𝑘𝑊(𝐴_𝑖,\overline{𝐴}_i)
$$
其中$\overline{𝐴}_i$为$A_i$的补集，意为除$A_i$子集外其他 V 的子集的并集。

我们选择一个权重最小的边缘的点，以最小化 $cut(A_1,A_2,...A_k)$ ，但是却不一定是最优的切图，如何避免这种切图，并且找到类似图中"Best Cut"这样的最优切图，即为谱聚类要解决的问题。



## 切图聚类

为了避免最小切图导致的切图效果不佳，我们需要对每个子图的规模做出限定，一般来说，有两种切图方式，第一种是RatioCut，第二种是Ncut。

### RatioCut

RatioCut切图为了避免第五节的最小切图，对每个切图，不光考虑最小化 $cut(A_1,A_2,...A_k)$，它还同时考虑最大化每个子图点的个数，即：
$$
RatioC𝑢𝑡(𝐴_1,𝐴_2,...𝐴_𝑘)=\frac{1}{2}\sum_{i=1}^𝑘\frac{𝑊(𝐴_𝑖,\overline{𝐴}_i)}{|A_i|}
$$
该公式在cut的公式基础上，除了一个切图的数量，即在最小化RatioCut时，可以将切图大小考虑进去。

引入指示向量$ℎ_𝑗\in\{ℎ_1,ℎ_2,..ℎ_𝑘\}$,

定义h~ij~：
$$
ℎ_{𝑖𝑗} =
\begin{cases}
0 \quad \quad v_i \notin A_j\\
\frac{1}{|𝐴_𝑗|} \quad 𝑣_𝑖\in𝐴_𝑗
\end{cases}
$$
关于 h~ij~ 的举例见下图：

 <img src="https://shiy-d.github.io/ImageHost/切图.jpeg" alt="切图" style="zoom: 40%;" />

根据拉普拉斯矩阵的性质3 可得，
$$
ℎ^𝑇_𝑖𝐿ℎ_𝑖 = \frac{𝑐𝑢𝑡(𝐴_𝑖,\overline{𝐴}_𝑖)}{|𝐴_𝑖|}
$$
由此，RatioCut就可以替换成带指示向量 h~ij~ 的公式：
$$
𝑅𝑎𝑡𝑖𝑜𝐶𝑢𝑡(𝐴_1,𝐴_2,...𝐴_𝑘)=\sum_{𝑖=1}^𝑘ℎ^𝑇_𝑖𝐿ℎ_𝑖=\sum_{𝑖=1}^𝑘(𝐻^𝑇𝐿𝐻)_{𝑖𝑖}=t𝑟(𝐻^𝑇𝐿𝐻)
$$
其中$𝑡𝑟(𝐻𝑇^𝐿𝐻)$为矩阵的迹。也就是说，RatioCut切图，实际上就是最小化$𝑡𝑟(𝐻^𝑇𝐿𝐻)$。由于$𝐻^𝑇𝐻=𝐼$，则切图优化目标变为:
$$
\underbrace{argmin}_H \,𝑡𝑟(𝐻^𝑇𝐿𝐻)\quad𝑠.𝑡.𝐻^𝑇𝐻=𝐼
$$
由上面的 h~ij~ 的定义可知，H 矩阵内各变量的取值只有 “0” 和 “$\frac{1}{|A_i|}$” 两种可能，那对于每个子图的 h^j^ 就有 2^n^ 种可能，整个 H 矩阵就有k·2^n^ 种可能。这样对于求最优解，是一个NP难的问题。

但由于，$𝑡𝑟(𝐻^𝑇𝐿𝐻)$ 中每一个优化子目标 $ℎ^𝑇_𝑖𝐿ℎ_𝑖$ ,其中 ℎ 是单位正交基， L 为对称矩阵，<u>此时 $ℎ^𝑇_𝑖𝐿ℎ_𝑖$ 的最大值即为 L 的最大特征值，最小值即为 L 的最小特征值</u>。那==问题就转为求矩阵 L 的最小特征值==。（此处可以联系，奇异值分解SVD，它是求L的最大值）

对于 $ℎ^𝑇_𝑖𝐿ℎ_𝑖$ ，问题目标是找到最小的 L 的特征值，而对于$𝑡𝑟(𝐻^𝑇𝐿𝐻)=\sum_{𝑖=1}^𝑘ℎ^𝑇_𝑖𝐿ℎ_𝑖$，则问题目标就是找到 k。个最小的特征值，一般来说，k远远小于n，也就是说，此时进行了**==维度规约==**，<u>将维度从n降到了k，从而近似可以解决这个NP难的问题</u>。

通过找到 L 的最小的 k 个特征值，可以得到对应的 k 个特征向量，这 k 个特征向量组成一个 `n x k` 维度的矩阵，即为要求解的 H。一般需要对 H 矩阵按行再做标准化，即
$$
ℎ^∗_{𝑖𝑗}=\frac{ℎ_{𝑖𝑗}}{(\sum_{𝑡=1}^𝑘ℎ^2_{𝑖𝑡})^{1/2}}
$$
由于我们在使用维度规约的时候损失了少量信息，导致得到的优化后的指示向量h对应的 H 现在不能完全指示各样本的归属，因此一般在得到 `n x k`维度的矩阵 H 后还需要对每一行进行一次传统的聚类，比如使用 K-Means聚类。

下图，我整理了一下谱聚类中 RatioCut 的思路：

 <img src="https://shiy-d.github.io/ImageHost/谱聚类.jpg" alt="谱聚类" style="zoom:30%;" />

### Ncut

之后再学吧...

RatioCut 已经把我整没了



参考：https://www.cnblogs.com/pinard/p/6221564.html



